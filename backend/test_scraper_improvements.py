"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ OZON —Å–∫—Ä–∞–ø–µ—Ä–∞

–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç:
1. –ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∑–∞—â–∏—Ç–æ–π –æ—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏
2. –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –∏ –æ—Ç–∑—ã–≤–æ–≤
3. –û–±—Ö–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ OZON

Usage:
    python test_scraper_improvements.py
"""

import asyncio
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º backend –≤ path
backend_dir = Path(__file__).parent
sys.path.insert(0, str(backend_dir))

from services.ozon_scraper import OzonScraper
from loguru import logger


async def test_improved_scraper():
    """–¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–µ—Ä–∞ —Å anti-detection"""

    logger.info("=" * 80)
    logger.info("üß™ Testing Improved OZON Scraper (Anti-Detection)")
    logger.info("=" * 80)

    scraper = OzonScraper(
        cache_ttl=3600,
        rate_limit=5,  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ª–∏–º–∏—Ç
        timeout=30
    )

    try:
        # –¢–µ—Å—Ç–æ–≤—ã–µ –∞—Ä—Ç–∏–∫—É–ª—ã
        test_articles = [
            "1066650955",  # –ê—Ä—Ç–∏–∫—É–ª –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        ]

        logger.info(f"Testing {len(test_articles)} articles with improved scraper\n")

        successful = 0
        failed = 0

        for idx, article in enumerate(test_articles, 1):
            logger.info("")
            logger.info("=" * 80)
            logger.info(f"[{idx}/{len(test_articles)}] Testing article: {article}")
            logger.info("=" * 80)

            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º force_playwright –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –∑–∞—â–∏—Ç—ã
                product = await scraper.get_product_info(
                    article=article,
                    force_playwright=True,  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º Playwright
                    use_cache=False  # –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à –¥–ª—è —Ç–µ—Å—Ç–∞
                )

                if product:
                    successful += 1
                    logger.success(f"‚úÖ Successfully scraped article: {article}")
                    logger.info("")
                    logger.info(f"üì¶ Product Details:")
                    logger.info(f"   Name: {product.name}")
                    logger.info(f"   Article: {product.article}")
                    logger.info(f"   Price: {product.price} ‚ÇΩ" if product.price else "   Price: Not available")
                    logger.info(f"   Normal Price: {product.normal_price} ‚ÇΩ" if product.normal_price else "   Normal Price: Not available")
                    logger.info(f"   Ozon Card Price: {product.ozon_card_price} ‚ÇΩ" if product.ozon_card_price else "   Ozon Card Price: Not available")
                    logger.info(f"   Rating: {product.rating} ‚≠ê" if product.rating else "   Rating: No rating")
                    logger.info(f"   Reviews: {product.reviews_count}" if product.reviews_count else "   Reviews: No reviews")
                    logger.info(f"   Availability: {product.availability.value}")
                    logger.info(f"   Source: {product.source.value}")
                    logger.info(f"   Fetch time: {product.fetch_time_ms}ms" if product.fetch_time_ms else "")
                    logger.info(f"   URL: {product.url}")
                else:
                    failed += 1
                    logger.warning(f"‚ö†Ô∏è  Product not found: {article}")

            except Exception as e:
                failed += 1
                logger.error(f"‚ùå Failed to scrape {article}: {e}")
                import traceback
                logger.debug(traceback.format_exc())

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if idx < len(test_articles):
                logger.info("Waiting before next request...")
                await asyncio.sleep(2)

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info("")
        logger.info("=" * 80)
        logger.info("üìä Test Results:")
        logger.info("=" * 80)
        logger.info(f"   Total articles tested: {len(test_articles)}")
        logger.info(f"   ‚úÖ Successful: {successful}")
        logger.info(f"   ‚ùå Failed: {failed}")
        logger.info(f"   Success rate: {(successful/len(test_articles)*100):.1f}%")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∫—Ä–∞–ø–µ—Ä–∞
        logger.info("")
        logger.info("=" * 80)
        scraper.print_stats()
        logger.info("=" * 80)

        if successful > 0:
            logger.success(f"‚úÖ Test completed! {successful}/{len(test_articles)} products scraped successfully")
        else:
            logger.error("‚ùå All tests failed - check OZON protection or network connection")

    except Exception as e:
        logger.error(f"‚ùå Test suite failed: {e}")
        import traceback
        logger.error(traceback.format_exc())

    finally:
        await scraper.close()
        logger.info("üîí Scraper closed")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger.remove()
    logger.add(
        sys.stdout,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO"
    )

    logger.info("üöÄ Starting OZON Scraper Improvement Tests")
    logger.info("")
    logger.info("This test validates:")
    logger.info("  - Anti-detection improvements (slow_mo, additional browser flags)")
    logger.info("  - Enhanced rating and reviews parsing (6+ selectors)")
    logger.info("  - Ability to bypass OZON protection")
    logger.info("")

    await test_improved_scraper()

    logger.info("")
    logger.info("‚úÖ All tests completed!")


if __name__ == "__main__":
    asyncio.run(main())
