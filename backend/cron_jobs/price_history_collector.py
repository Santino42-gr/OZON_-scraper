"""
Price History Collector - Cron Job

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä –∏—Å—Ç–æ—Ä–∏–∏ —Ü–µ–Ω —Ç–æ–≤–∞—Ä–æ–≤ OZON —á–µ—Ä–µ–∑ Parser Market API.

–ó–∞–ø—É—Å–∫: –ö–∞–∂–¥—ã–µ 24 —á–∞—Å–∞ (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è)
–¶–µ–ª—å: –°–æ–±—Ä–∞—Ç—å —Ü–µ–Ω—ã –≤—Å–µ—Ö –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö –∞—Ä—Ç–∏–∫—É–ª–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: –î–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ä–µ–¥–Ω–µ–π —Ü–µ–Ω—ã –∑–∞ 7 –¥–Ω–µ–π

Usage:
    python -m cron_jobs.price_history_collector

Environment Variables:
    SUPABASE_URL - URL Supabase –ø—Ä–æ–µ–∫—Ç–∞
    SUPABASE_SERVICE_ROLE_KEY - Service role –∫–ª—é—á –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ –ë–î
    PARSER_MARKET_API_KEY - API –∫–ª—é—á Parser Market
    OZON_SCRAPER_BATCH_SIZE - –†–∞–∑–º–µ—Ä batch –¥–ª—è scraping (default: 10)
    OZON_SCRAPER_DELAY - –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (default: 2)
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict, Any
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º backend –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger
from database import get_supabase_client
from services.parser_market_client import ParserMarketClient
from config import settings


class PriceHistoryCollector:
    """
    –°–±–æ—Ä—â–∏–∫ –∏—Å—Ç–æ—Ä–∏–∏ —Ü–µ–Ω –¥–ª—è –≤—Å–µ—Ö –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö –∞—Ä—Ç–∏–∫—É–ª–æ–≤
    
    –ê–ª–≥–æ—Ä–∏—Ç–º:
    1. –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∞—Ä—Ç–∏–∫—É–ª—ã –∏–∑ –ë–î
    2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∞—Ä—Ç–∏–∫—É–ª–∞: scrape —Ç–µ–∫—É—â—É—é —Ü–µ–Ω—É
    3. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ ozon_scraper_price_history
    4. –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –æ—à–∏–±–∫–∏
    """
    
    def __init__(self, batch_size: int = 10, delay_seconds: int = 2):
        """
        Args:
            batch_size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞—Ä—Ç–∏–∫—É–ª–æ–≤ –≤ –æ–¥–Ω–æ–º batch
            delay_seconds: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏ (–¥–ª—è rate limiting)
        """
        self.batch_size = batch_size
        self.delay_seconds = delay_seconds
        self.client = None
        self.supabase = get_supabase_client()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        self.stats = {
            "total_articles": 0,
            "successful": 0,
            "failed": 0,
            "skipped": 0,
            "start_time": None,
            "end_time": None,
            "errors": []
        }

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Parser Market client"""
        self.client = ParserMarketClient(
            api_key=settings.PARSER_MARKET_API_KEY,
            region=settings.PARSER_MARKET_REGION,
            timeout=settings.PARSER_MARKET_TIMEOUT,
            poll_interval=settings.PARSER_MARKET_POLL_INTERVAL
        )
        logger.info("Parser Market client initialized for cron job")

    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.client:
            await self.client.close()
        logger.info("Resources cleaned up")
    
    def get_all_articles(self) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∞—Ä—Ç–∏–∫—É–ª—ã –∏–∑ –ë–î
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∞—Ä—Ç–∏–∫—É–ª–æ–≤
        """
        try:
            response = self.supabase.table("ozon_scraper_articles") \
                .select("article_number") \
                .eq("status", "active") \
                .execute()
            
            articles = [row["article_number"] for row in response.data]
            unique_articles = list(set(articles))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            
            logger.info(f"Found {len(unique_articles)} unique articles to scrape")
            return unique_articles
            
        except Exception as e:
            logger.error(f"Failed to fetch articles from DB: {e}")
            return []
    
    async def scrape_article_price(self, article: str) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ü–µ–Ω—É —Ç–æ–≤–∞—Ä–∞ —á–µ—Ä–µ–∑ Parser Market API

        Args:
            article: –ê—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –æ —Ü–µ–Ω–∞—Ö –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Parser Market API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
            product_info = await self.client.parse_sync(article)

            if not product_info:
                logger.warning(f"No data found for article: {article}")
                return None

            # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ –ë–î
            price_data = {
                "article_number": article,
                "price": product_info.price,
                "normal_price": product_info.normal_price,
                "ozon_card_price": product_info.ozon_card_price,
                "old_price": product_info.old_price,
                "product_available": product_info.available,
                "rating": product_info.rating,
                "reviews_count": product_info.reviews_count,
                "source": "parser_market_api",
                "scraping_success": True,
                "scraping_duration_ms": product_info.fetch_time_ms,
                "price_date": datetime.now().isoformat()
            }

            logger.info(
                f"‚úÖ Successfully parsed {article}: price={price_data['price']}"
            )
            return price_data

        except Exception as e:
            logger.error(f"‚ùå Failed to parse {article}: {e}")
            self.stats["errors"].append({
                "article": article,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })
            return None
    
    def save_to_database(self, price_data: Dict[str, Any]) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–µ –≤ –ë–î
        
        Args:
            price_data: –î–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–µ
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ
        """
        try:
            self.supabase.table("ozon_scraper_price_history") \
                .insert(price_data) \
                .execute()
            
            logger.debug(f"Saved price history for {price_data['article_number']}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to save to DB: {e}")
            return False
    
    async def process_batch(self, articles: List[str]):
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å batch –∞—Ä—Ç–∏–∫—É–ª–æ–≤
        
        Args:
            articles: –°–ø–∏—Å–æ–∫ –∞—Ä—Ç–∏–∫—É–ª–æ–≤
        """
        for article in articles:
            # Scrape —Ü–µ–Ω—É
            price_data = await self.scrape_article_price(article)
            
            if price_data:
                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î
                if self.save_to_database(price_data):
                    self.stats["successful"] += 1
                else:
                    self.stats["failed"] += 1
            else:
                self.stats["failed"] += 1
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (rate limiting)
            await asyncio.sleep(self.delay_seconds)
    
    async def run(self):
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥: –∑–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Ü–µ–Ω
        """
        self.stats["start_time"] = datetime.now()
        
        logger.info("="*60)
        logger.info("üöÄ Starting Price History Collection Cron Job")
        logger.info("="*60)
        
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            await self.initialize()
            
            # –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∞—Ä—Ç–∏–∫—É–ª—ã
            articles = self.get_all_articles()
            self.stats["total_articles"] = len(articles)
            
            if not articles:
                logger.warning("No articles found to process")
                return
            
            logger.info(f"Processing {len(articles)} articles in batches of {self.batch_size}")
            
            # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∞—Ä—Ç–∏–∫—É–ª—ã batch-–∞–º–∏
            for i in range(0, len(articles), self.batch_size):
                batch = articles[i:i + self.batch_size]
                batch_num = (i // self.batch_size) + 1
                total_batches = (len(articles) + self.batch_size - 1) // self.batch_size
                
                logger.info(f"Processing batch {batch_num}/{total_batches} ({len(batch)} articles)")
                await self.process_batch(batch)
            
            # –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
            self.stats["end_time"] = datetime.now()
            duration = (self.stats["end_time"] - self.stats["start_time"]).total_seconds()
            
            logger.info("="*60)
            logger.info("‚úÖ Price History Collection Completed")
            logger.info("="*60)
            logger.info(f"Total articles: {self.stats['total_articles']}")
            logger.info(f"Successful: {self.stats['successful']}")
            logger.info(f"Failed: {self.stats['failed']}")
            logger.info(f"Duration: {duration:.2f}s")
            logger.info(f"Success rate: {(self.stats['successful'] / max(self.stats['total_articles'], 1) * 100):.1f}%")
            
            if self.stats["errors"]:
                logger.warning(f"Errors encountered: {len(self.stats['errors'])}")
                for error in self.stats["errors"][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫
                    logger.warning(f"  - {error['article']}: {error['error']}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –≤ –ë–î
            self.log_cron_execution()
            
        except Exception as e:
            logger.critical(f"Cron job failed with critical error: {e}")
            import traceback
            traceback.print_exc()
            
        finally:
            await self.cleanup()
    
    def log_cron_execution(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è cron job –≤ –ë–î"""
        try:
            log_entry = {
                "level": "INFO" if self.stats["failed"] == 0 else "WARNING",
                "event_type": "cron_price_history_collection",
                "message": f"Price history collection completed: {self.stats['successful']}/{self.stats['total_articles']} successful",
                "metadata": {
                    "stats": self.stats,
                    "batch_size": self.batch_size,
                    "delay_seconds": self.delay_seconds
                }
            }
            
            self.supabase.table("ozon_scraper_logs") \
                .insert(log_entry) \
                .execute()
            
            logger.info("Cron execution logged to database")
            
        except Exception as e:
            logger.error(f"Failed to log cron execution: {e}")


async def main():
    """
    Entry point –¥–ª—è cron job
    """
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ environment variables
    batch_size = int(os.getenv("OZON_SCRAPER_BATCH_SIZE", "10"))
    delay = int(os.getenv("OZON_SCRAPER_DELAY", "5"))
    
    collector = PriceHistoryCollector(batch_size=batch_size, delay_seconds=delay)
    await collector.run()


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ cron job
    asyncio.run(main())

