"""
OZON Scraper Service

Web scraping –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–≤–∞—Ä–∞—Ö OZON —á–µ—Ä–µ–∑ Playwright.

Features:
- Playwright –¥–ª—è –æ–±—Ö–æ–¥–∞ –∞–Ω—Ç–∏–±–æ—Ç —Å–∏—Å—Ç–µ–º
- httpx + BeautifulSoup –∫–∞–∫ fallback
- Retry –ª–æ–≥–∏–∫–∞ —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º backoff
- Rate limiting (Sliding Window)
- In-memory –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ (TTL 1 —á–∞—Å)
- –ü–æ–ª–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –ë–î

Author: AI Agent
Created: 2025-10-20
"""

import asyncio
import time
import uuid
import traceback
import random
from typing import Optional, Dict, List, Any
from datetime import datetime, timedelta
from collections import deque
from urllib.parse import quote

import httpx
from bs4 import BeautifulSoup
from loguru import logger

# Pydantic –º–æ–¥–µ–ª–∏
from models.ozon_models import (
    ProductInfo,
    ProductPriceDetailed,
    ProductStock,
    ProductRating,
    ProductAvailability,
    ScrapingSource,
    ScrapingResult,
    OzonRequestLog
)

# Config
from config import settings


# ==================== Rate Limiter ====================

class RateLimiter:
    """
    Rate Limiter –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º "Sliding Window" –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è RPS.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è.
    """
    
    def __init__(self, max_requests: int = 10, time_window: int = 60):
        """
        Args:
            max_requests: –ú–∞–∫—Å–∏–º—É–º –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ time_window
            time_window: –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 60)
        """
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests: deque = deque()
        self._lock = asyncio.Lock()
    
    async def acquire(self):
        """
        –ñ–¥–∞—Ç—å –ø–æ–∫–∞ –Ω–µ –æ—Å–≤–æ–±–æ–¥–∏—Ç—Å—è —Å–ª–æ—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        
        –ë–ª–æ–∫–∏—Ä—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤.
        –î–æ–±–∞–≤–ª—è–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞.
        """
        async with self._lock:
            now = time.time()
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–≤–Ω–µ –æ–∫–Ω–∞)
            while self.requests and self.requests[0] < now - self.time_window:
                self.requests.popleft()
            
            # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞ - –∂–¥–µ–º
            if len(self.requests) >= self.max_requests:
                sleep_time = self.time_window - (now - self.requests[0]) + 1
                if sleep_time > 0:
                    logger.warning(
                        f"‚ö†Ô∏è  Rate limit reached ({len(self.requests)}/{self.max_requests}). "
                        f"Sleeping for {sleep_time:.1f}s"
                    )
                    await asyncio.sleep(sleep_time)
                    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞
                    return await self.acquire()
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É (1-3 —Å–µ–∫) –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞
            random_delay = random.uniform(1.0, 3.0)
            if len(self.requests) > 0:
                await asyncio.sleep(random_delay)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å
            self.requests.append(time.time())
            
            logger.debug(
                f"Rate limiter: {len(self.requests)}/{self.max_requests} requests "
                f"in window (delay: {random_delay:.2f}s)"
            )


# ==================== Retry Logic ====================

async def retry_with_backoff(
    func,
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 10.0,
    exceptions: tuple = (Exception,)
):
    """
    Retry decorator —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º backoff
    
    Args:
        func: Async —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        max_retries: –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫
        base_delay: –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ (—Å–µ–∫)
        max_delay: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ (—Å–µ–∫)
        exceptions: Tuple –∏—Å–∫–ª—é—á–µ–Ω–∏–π –¥–ª—è retry
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏
        
    Raises:
        –ü–æ—Å–ª–µ–¥–Ω–µ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –µ—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ failed
    """
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            return await func()
        except exceptions as e:
            last_exception = e
            
            if attempt == max_retries - 1:
                # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É
                raise
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º —Ä–æ—Å—Ç–æ–º + jitter
            delay = min(base_delay * (2 ** attempt), max_delay)
            jitter = random.uniform(0, delay * 0.1)  # 10% jitter
            total_delay = delay + jitter
            
            logger.warning(
                f"üîÑ Attempt {attempt + 1}/{max_retries} failed: {type(e).__name__}: {str(e)}. "
                f"Retrying in {total_delay:.2f}s..."
            )
            
            await asyncio.sleep(total_delay)
    
    # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
    raise last_exception


# ==================== OZON Scraper ====================

class OzonScraper:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è web scraping OZON
    
    Features:
    - Playwright –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥)
    - httpx + BeautifulSoup –∫–∞–∫ fallback
    - Retry –ª–æ–≥–∏–∫–∞ —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º backoff
    - Rate limiting (Sliding Window)
    - In-memory –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ (TTL 1 —á–∞—Å)
    - –ü–æ–ª–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –ë–î
    
    Usage:
        scraper = OzonScraper()
        product = await scraper.get_product_info("123456789")
        await scraper.close()
    """
    
    def __init__(
        self,
        cache_ttl: int = 3600,  # 1 —á–∞—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        rate_limit: int = 10,
        timeout: int = 30
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞
        
        Args:
            cache_ttl: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (default: 3600 = 1 —á–∞—Å)
            rate_limit: –ú–∞–∫—Å–∏–º—É–º –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–∏–Ω—É—Ç—É
            timeout: Timeout –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        self.base_url = "https://www.ozon.ru"
        self.cache_ttl = cache_ttl
        self._cache: Dict[str, ProductInfo] = {}
        self._playwright = None
        self._browser = None
        self._context = None
        
        # Rate limiter
        self.rate_limiter = RateLimiter(max_requests=rate_limit, time_window=60)
        
        # HTTP –∫–ª–∏–µ–Ω—Ç (fallback)
        self.client = httpx.AsyncClient(
            timeout=timeout,
            follow_redirects=True,
            headers=self._get_default_headers()
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "playwright_requests": 0,
            "httpx_requests": 0
        }
        
        logger.info(
            f"üöÄ OzonScraper initialized: cache_ttl={cache_ttl}s, "
            f"rate_limit={rate_limit}/min, timeout={timeout}s"
        )
    
    def _get_default_headers(self) -> Dict[str, str]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ HTTP –∑–∞–≥–æ–ª–æ–≤–∫–∏"""
        return {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Cache-Control": "max-age=0"
        }
    
    async def _init_playwright(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Playwright (lazy loading)"""
        if self._playwright is None:
            try:
                from playwright.async_api import async_playwright
                
                self._playwright = await async_playwright().start()
                self._browser = await self._playwright.chromium.launch(
                    headless=getattr(settings, 'PLAYWRIGHT_HEADLESS', True)
                )
                self._context = await self._browser.new_context(
                    viewport={"width": 1920, "height": 1080},
                    user_agent=self._get_default_headers()["User-Agent"]
                )
                
                logger.info("‚úÖ Playwright initialized successfully")
                
            except ImportError:
                logger.error(
                    "‚ùå Playwright not installed! "
                    "Run: pip install playwright && playwright install chromium"
                )
                raise
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize Playwright: {e}")
                raise
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å –≤—Å–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.client:
            await self.client.aclose()
        
        if self._context:
            await self._context.close()
        
        if self._browser:
            await self._browser.close()
        
        if self._playwright:
            await self._playwright.stop()
        
        logger.info("üîí OzonScraper closed")
    
    # ==================== –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ ====================
    
    def _get_from_cache(self, article: str) -> Optional[ProductInfo]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞"""
        if article in self._cache:
            product = self._cache[article]
            age = (datetime.now() - product.last_check).total_seconds()
            
            if age < self.cache_ttl:
                self.stats["cache_hits"] += 1
                logger.info(f"üíæ Cache HIT for {article} (age: {age:.1f}s)")
                return product
            else:
                logger.debug(f"‚è∞ Cache EXPIRED for {article} (age: {age:.1f}s)")
                del self._cache[article]
        
        self.stats["cache_misses"] += 1
        return None
    
    def _save_to_cache(self, article: str, product: ProductInfo):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à"""
        product.last_check = datetime.now()
        self._cache[article] = product
        logger.debug(f"üíæ Cached product {article}")
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –≤–µ—Å—å –∫—ç—à"""
        count = len(self._cache)
        self._cache.clear()
        logger.info(f"üßπ Cache cleared: {count} entries removed")
    
    # ==================== –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ====================
    
    async def _log_request(self, log: OzonRequestLog):
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–≥ –∑–∞–ø—Ä–æ—Å–∞ –≤ –ë–î
        
        Args:
            log: OzonRequestLog model
        """
        try:
            from database import get_supabase_client
            
            supabase = get_supabase_client()
            
            log_data = {
                "level": "error" if not log.success else "info",
                "event_type": f"ozon_{log.method}",
                "message": (
                    f"OZON {log.method} for {log.article}: "
                    f"{'SUCCESS' if log.success else 'FAILED'}"
                ),
                "metadata": {
                    "request_id": log.request_id,
                    "method": log.method,
                    "article": log.article,
                    "success": log.success,
                    "status_code": log.status_code,
                    "duration_ms": log.duration_ms,
                    "retry_count": log.retry_count,
                    "source": log.source.value,
                    "cache_hit": log.cache_hit,
                    "error_message": log.error_message,
                    "error_traceback": log.error_traceback
                },
                "created_at": log.start_time.isoformat()
            }
            
            supabase.table("ozon_scraper_logs").insert(log_data).execute()
            logger.debug(f"üìù Request log saved: {log.request_id}")
            
        except Exception as e:
            # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º —Ä–∞–±–æ—Ç—É –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–≥
            logger.warning(f"‚ö†Ô∏è  Failed to save request log to DB: {e}")
    
    # ==================== URL Construction ====================
    
    def _construct_product_url(self, article: str) -> str:
        """
        –°–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å URL —Ç–æ–≤–∞—Ä–∞ –Ω–∞ OZON
        
        Args:
            article: –∞—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä–∞
            
        Returns:
            URL –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–∞
        """
        # OZON search URL
        return f"{self.base_url}/search/?text={quote(article)}"
    
    # ==================== HTML Parsing ====================
    
    def _parse_product_from_html(self, html: str, article: str) -> Optional[ProductInfo]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã OZON –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–≤–∞—Ä–µ
        
        Args:
            html: HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            article: –∞—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä–∞
            
        Returns:
            ProductInfo –∏–ª–∏ None
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ò—â–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ (–æ–±—ã—á–Ω–æ —ç—Ç–æ –Ω–∞—à —Ç–æ–≤–∞—Ä)
            search_results = soup.find_all('div', {'data-widget': 'searchResultsV2'})
            if not search_results:
                logger.warning(f"‚ö†Ô∏è  No search results found for {article}")
                return None
            
            # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫—É —Ç–æ–≤–∞—Ä–∞
            product_card = search_results[0].find('div', recursive=True)
            if not product_card:
                logger.warning(f"‚ö†Ô∏è  No product card found for {article}")
                return None
            
            # === –ü–∞—Ä—Å–∏–Ω–≥ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞ ===
            name = None
            name_elem = soup.find('span', class_=lambda x: x and 'tsBody500Medium' in x)
            if name_elem:
                name = name_elem.get_text(strip=True)
            
            # === –ü–∞—Ä—Å–∏–Ω–≥ —Ü–µ–Ω ===
            # 1. –û–±—ã—á–Ω–∞—è —Ü–µ–Ω–∞ (–±–µ–∑ Ozon Card) - —á–µ—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç
            normal_price = None
            normal_price_elem = soup.find('span', {'data-widget': 'webPrice'})
            if not normal_price_elem:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä
                normal_price_elem = soup.find('span', class_=lambda x: x and 'tsHeadline500Medium' in x)
            
            if normal_price_elem:
                price_text = normal_price_elem.get_text(strip=True)
                normal_price = self._parse_price_text(price_text)
            
            # 2. –¶–µ–Ω–∞ —Å Ozon Card - —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
            ozon_card_price = None
            ozon_card_elem = soup.find('span', {'data-widget': 'webOzonCardPrice'})
            if not ozon_card_elem:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä
                ozon_card_elem = soup.find('span', class_=lambda x: x and 'ozonCard' in str(x).lower())
            
            if ozon_card_elem:
                price_text = ozon_card_elem.get_text(strip=True)
                ozon_card_price = self._parse_price_text(price_text)
            
            # 3. –°—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞ (–ø–µ—Ä–µ—á–µ—Ä–∫–Ω—É—Ç–∞—è)
            old_price = None
            old_price_elem = soup.find('span', class_=lambda x: x and 'line-through' in str(x))
            if not old_price_elem:
                old_price_elem = soup.find('s')
            
            if old_price_elem:
                price_text = old_price_elem.get_text(strip=True)
                old_price = self._parse_price_text(price_text)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ü–µ–Ω—É
            price = ozon_card_price or normal_price
            
            # === –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–π—Ç–∏–Ω–≥–∞ ===
            rating = None
            rating_elem = soup.find('span', class_=lambda x: x and 'rating' in str(x).lower())
            if rating_elem:
                rating_text = rating_elem.get_text(strip=True)
                try:
                    rating = float(rating_text.replace(',', '.'))
                except:
                    pass
            
            # === –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ—Ç–∑—ã–≤–æ–≤ ===
            reviews_count = None
            reviews_elem = soup.find('span', string=lambda x: x and '–æ—Ç–∑—ã–≤' in str(x).lower())
            if reviews_elem:
                reviews_text = reviews_elem.get_text(strip=True)
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ç–∏–ø–∞ "123 –æ—Ç–∑—ã–≤–∞"
                    import re
                    match = re.search(r'\d+', reviews_text)
                    if match:
                        reviews_count = int(match.group())
                except:
                    pass
            
            # === –ü–∞—Ä—Å–∏–Ω–≥ URL —Ç–æ–≤–∞—Ä–∞ ===
            product_url = None
            link_elem = soup.find('a', href=lambda x: x and '/product/' in str(x))
            if link_elem:
                href = link_elem.get('href')
                if href.startswith('http'):
                    product_url = href
                elif href.startswith('/'):
                    product_url = f"{self.base_url}{href}"
            
            # === –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ===
            image_url = None
            img_elem = soup.find('img', src=lambda x: x and 'cdn' in str(x).lower())
            if img_elem:
                image_url = img_elem.get('src')
            
            # === –ü–∞—Ä—Å–∏–Ω–≥ –Ω–∞–ª–∏—á–∏—è ===
            availability = ProductAvailability.AVAILABLE
            if soup.find(string=lambda x: x and '–Ω–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏' in str(x).lower()):
                availability = ProductAvailability.OUT_OF_STOCK
            elif soup.find(string=lambda x: x and '–ø–æ–¥ –∑–∞–∫–∞–∑' in str(x).lower()):
                availability = ProductAvailability.PRE_ORDER
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –Ω–∏ –æ–¥–Ω—É —Ü–µ–Ω—É - —Ç–æ–≤–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω
            if not price and not normal_price:
                logger.warning(f"‚ö†Ô∏è  No prices found for {article}")
                return None
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            product = ProductInfo(
                article=article,
                name=name or f"Product {article}",
                price=price,
                normal_price=normal_price,
                ozon_card_price=ozon_card_price,
                old_price=old_price,
                average_price_7days=None,  # –ë—É–¥–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–æ –ø–æ–∑–∂–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏
                rating=rating,
                reviews_count=reviews_count,
                availability=availability,
                image_url=image_url,
                url=product_url or self._construct_product_url(article),
                source=ScrapingSource.HTTPX
            )
            
            logger.info(
                f"‚úÖ Parsed {article}: "
                f"normal_price={normal_price}, "
                f"ozon_card_price={ozon_card_price}, "
                f"name={name}"
            )
            
            return product
            
        except Exception as e:
            logger.error(f"‚ùå Error parsing HTML for {article}: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return None
    
    def _parse_price_text(self, price_text: str) -> Optional[float]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            price_text: –¢–µ–∫—Å—Ç —Å —Ü–µ–Ω–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, "1 999 ‚ÇΩ", "1999", "1,999.00")
            
        Returns:
            –¶–µ–Ω–∞ –∫–∞–∫ float –∏–ª–∏ None
        """
        try:
            # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä, —Ç–æ—á–µ–∫ –∏ –∑–∞–ø—è—Ç—ã—Ö
            import re
            cleaned = re.sub(r'[^\d,.]', '', price_text)
            
            # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É
            cleaned = cleaned.replace(',', '.')
            
            # –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–µ–∫, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω—é—é (–¥–µ—Å—è—Ç–∏—á–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å)
            parts = cleaned.split('.')
            if len(parts) > 2:
                cleaned = ''.join(parts[:-1]) + '.' + parts[-1]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
            price = float(cleaned)
            
            return price if price > 0 else None
            
        except Exception as e:
            logger.debug(f"Failed to parse price from '{price_text}': {e}")
            return None
    
    # ==================== –°–ü–ü –ú–µ—Ç—Ä–∏–∫–∏ ====================
    
    @staticmethod
    def calculate_spp_metrics(
        average_price_7days: Optional[float],
        normal_price: Optional[float],
        ozon_card_price: Optional[float]
    ) -> Dict[str, Optional[float]]:
        """
        –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –°–ü–ü (—Å–∫–∏–¥–∫–∏)
        
        –§–æ—Ä–º—É–ª—ã:
        - –°–ü–ü1 = (–°—Ä–µ–¥–Ω—è—è –∑–∞ 7 –¥–Ω–µ–π - –û–±—ã—á–Ω–∞—è —Ü–µ–Ω–∞) / –°—Ä–µ–¥–Ω—è—è –∑–∞ 7 –¥–Ω–µ–π √ó 100%
        - –°–ü–ü2 = (–û–±—ã—á–Ω–∞—è —Ü–µ–Ω–∞ - –¶–µ–Ω–∞ —Å –∫–∞—Ä—Ç–æ–π) / –û–±—ã—á–Ω–∞—è —Ü–µ–Ω–∞ √ó 100%
        - –°–ü–ü –û–±—â–∏–π = (–°—Ä–µ–¥–Ω—è—è –∑–∞ 7 –¥–Ω–µ–π - –¶–µ–Ω–∞ —Å –∫–∞—Ä—Ç–æ–π) / –°—Ä–µ–¥–Ω—è—è –∑–∞ 7 –¥–Ω–µ–π √ó 100%
        
        Args:
            average_price_7days: –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –∑–∞ 7 –¥–Ω–µ–π
            normal_price: –¶–µ–Ω–∞ –±–µ–∑ Ozon Card
            ozon_card_price: –¶–µ–Ω–∞ —Å Ozon Card
            
        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏ spp1, spp2, spp_total (–∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö –∏–ª–∏ None)
        """
        spp1 = None
        spp2 = None
        spp_total = None
        
        # –°–ü–ü1: (avg - normal) / avg * 100
        if average_price_7days and average_price_7days > 0 and normal_price is not None:
            spp1 = round((average_price_7days - normal_price) / average_price_7days * 100, 1)
        
        # –°–ü–ü2: (normal - card) / normal * 100
        if normal_price and normal_price > 0 and ozon_card_price is not None:
            spp2 = round((normal_price - ozon_card_price) / normal_price * 100, 1)
        
        # –°–ü–ü –û–±—â–∏–π: (avg - card) / avg * 100
        if average_price_7days and average_price_7days > 0 and ozon_card_price is not None:
            spp_total = round((average_price_7days - ozon_card_price) / average_price_7days * 100, 1)
        
        return {
            "spp1": spp1,
            "spp2": spp2,
            "spp_total": spp_total
        }
    
    # ==================== Scraping Methods ====================
    
    async def _scrape_with_playwright(self, article: str) -> Optional[ProductInfo]:
        """
        Scrape –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–≤–∞—Ä–µ –∏—Å–ø–æ–ª—å–∑—É—è Playwright
        
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ö–æ–¥–∞ –∞–Ω—Ç–∏–±–æ—Ç —Å–∏—Å—Ç–µ–º.
        
        Args:
            article: –∞—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä–∞
            
        Returns:
            ProductInfo –∏–ª–∏ None
        """
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Playwright –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
            await self._init_playwright()
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            page = await self._context.new_page()
            
            try:
                url = self._construct_product_url(article)
                logger.info(f"üåê Scraping {article} via Playwright: {url}")
                
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
                response = await page.goto(url, wait_until="domcontentloaded")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å
                if response.status != 200:
                    logger.warning(f"‚ö†Ô∏è  HTTP {response.status} for {article}")
                    if response.status == 403:
                        raise httpx.HTTPStatusError(
                            f"403 Forbidden for {url}",
                            request=None,
                            response=response
                        )
                
                # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–µ–ª–µ–∫—Ç–æ—Ä—ã)
                try:
                    await page.wait_for_selector('[data-widget="searchResultsV2"]', timeout=5000)
                except:
                    logger.warning(f"‚ö†Ô∏è  Timeout waiting for search results for {article}")
                
                # –ü–æ–ª—É—á–∞–µ–º HTML
                html = await page.content()
                
                # –ü–∞—Ä—Å–∏–º
                product = self._parse_product_from_html(html, article)
                
                if product:
                    product.source = ScrapingSource.PLAYWRIGHT
                    self.stats["playwright_requests"] += 1
                
                return product
                
            finally:
                await page.close()
                
        except Exception as e:
            logger.error(f"‚ùå Playwright scraping failed for {article}: {e}")
            raise
    
    async def _scrape_with_httpx(self, article: str) -> Optional[ProductInfo]:
        """
        Scrape –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–≤–∞—Ä–µ –∏—Å–ø–æ–ª—å–∑—É—è httpx (fallback –º–µ—Ç–æ–¥)
        
        Args:
            article: –∞—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä–∞
            
        Returns:
            ProductInfo –∏–ª–∏ None
        """
        try:
            url = self._construct_product_url(article)
            logger.info(f"üåê Scraping {article} via httpx: {url}")
            
            response = await self.client.get(url)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º HTML
            product = self._parse_product_from_html(response.text, article)
            
            if product:
                product.source = ScrapingSource.HTTPX
                self.stats["httpx_requests"] += 1
            
            return product
        
        except httpx.HTTPStatusError as e:
            logger.error(f"‚ùå HTTP {e.response.status_code} for {article}")
            
            # –ï—Å–ª–∏ 403 - –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ Playwright
            if e.response.status_code == 403:
                logger.info(f"üîÑ Switching to Playwright for {article}")
                return await self._scrape_with_playwright(article)
            
            raise
        
        except Exception as e:
            logger.error(f"‚ùå httpx scraping failed for {article}: {e}")
            return None
    
    # ==================== Public API ====================
    
    async def get_product_info(
        self,
        article: str,
        use_cache: bool = True,
        force_playwright: bool = False
    ) -> Optional[ProductInfo]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–≤–∞—Ä–µ
        
        Args:
            article: –∞—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä–∞ OZON
            use_cache: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à
            force_playwright: –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Playwright
            
        Returns:
            ProductInfo –∏–ª–∏ None –µ—Å–ª–∏ —Ç–æ–≤–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω
        """
        request_id = str(uuid.uuid4())
        start_time = datetime.now()
        cache_hit = False
        retry_count = 0
        
        self.stats["total_requests"] += 1
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—ç—à
        if use_cache:
            cached = self._get_from_cache(article)
            if cached:
                cache_hit = True
                
                # –õ–æ–≥–∏—Ä—É–µ–º cache hit
                await self._log_request(OzonRequestLog(
                    request_id=request_id,
                    method="get_product_info",
                    article=article,
                    success=True,
                    start_time=start_time,
                    end_time=datetime.now(),
                    duration_ms=int((datetime.now() - start_time).total_seconds() * 1000),
                    source=ScrapingSource.CACHE,
                    cache_hit=True,
                    retry_count=0
                ))
                
                return cached
        
        # Rate limiting
        await self.rate_limiter.acquire()
        
        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è retry
        async def fetch_product():
            nonlocal retry_count
            retry_count += 1
            
            if force_playwright:
                return await self._scrape_with_playwright(article)
            else:
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º httpx (–±—ã—Å—Ç—Ä–µ–µ)
                return await self._scrape_with_httpx(article)
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å retry
            product = await retry_with_backoff(
                fetch_product,
                max_retries=3,
                base_delay=1.0,
                exceptions=(httpx.TimeoutException, httpx.HTTPError)
            )
            
            end_time = datetime.now()
            duration_ms = int((end_time - start_time).total_seconds() * 1000)
            
            if product:
                product.fetch_time_ms = duration_ms
                
                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à
                if use_cache:
                    self._save_to_cache(article, product)
                
                self.stats["successful_requests"] += 1
                
                # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—Ö
                await self._log_request(OzonRequestLog(
                    request_id=request_id,
                    method="get_product_info",
                    article=article,
                    success=True,
                    status_code=200,
                    start_time=start_time,
                    end_time=end_time,
                    duration_ms=duration_ms,
                    source=product.source,
                    cache_hit=cache_hit,
                    retry_count=retry_count - 1
                ))
                
                logger.success(f"‚úÖ Successfully scraped {article} in {duration_ms}ms")
                
                return product
            else:
                self.stats["failed_requests"] += 1
                
                # –¢–æ–≤–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω
                await self._log_request(OzonRequestLog(
                    request_id=request_id,
                    method="get_product_info",
                    article=article,
                    success=False,
                    status_code=404,
                    start_time=start_time,
                    end_time=datetime.now(),
                    duration_ms=duration_ms,
                    error_message="Product not found",
                    source=ScrapingSource.HTTPX,
                    cache_hit=False,
                    retry_count=retry_count - 1
                ))
                
                logger.warning(f"‚ö†Ô∏è  Product not found: {article}")
                
                return None
        
        except Exception as e:
            end_time = datetime.now()
            duration_ms = int((end_time - start_time).total_seconds() * 1000)
            error_traceback = traceback.format_exc()
            
            self.stats["failed_requests"] += 1
            
            logger.error(f"‚ùå Error fetching product {article}: {e}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
            await self._log_request(OzonRequestLog(
                request_id=request_id,
                method="get_product_info",
                article=article,
                success=False,
                start_time=start_time,
                end_time=end_time,
                duration_ms=duration_ms,
                error_message=str(e),
                error_traceback=error_traceback,
                source=ScrapingSource.HTTPX,
                cache_hit=False,
                retry_count=retry_count - 1
            ))
            
            return None
    
    async def get_product_prices_detailed(self, article: str) -> Optional[ProductPriceDetailed]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ü–µ–Ω–∞—Ö —Ç–æ–≤–∞—Ä–∞
        
        –í–∫–ª—é—á–∞–µ—Ç —Ü–µ–Ω—ã —Å/–±–µ–∑ Ozon Card, —Å—Ç–∞—Ä—É—é —Ü–µ–Ω—É, —Å—Ä–µ–¥–Ω—é—é –∑–∞ 7 –¥–Ω–µ–π.
        
        Args:
            article: –∞—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä–∞
            
        Returns:
            ProductPriceDetailed –∏–ª–∏ None
        """
        product = await self.get_product_info(article)
        
        if not product:
            return None
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ä–µ–¥–Ω—é—é —Ü–µ–Ω—É –∑–∞ 7 –¥–Ω–µ–π –∏–∑ –ë–î
        average_price_7days = await self._get_average_price_from_db(article)
        
        return ProductPriceDetailed(
            article=article,
            price=product.price,
            normal_price=product.normal_price,
            ozon_card_price=product.ozon_card_price,
            old_price=product.old_price,
            average_price_7days=average_price_7days,
            last_updated=product.last_check
        )
    
    async def _get_average_price_from_db(self, article: str) -> Optional[float]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ä–µ–¥–Ω—é—é —Ü–µ–Ω—É –∑–∞ 7 –¥–Ω–µ–π –∏–∑ –ë–î
        
        Args:
            article: –∞—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä–∞
            
        Returns:
            –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –∏–ª–∏ None
        """
        try:
            from database import get_supabase_client
            
            supabase = get_supabase_client()
            
            # –í—ã–∑—ã–≤–∞–µ–º SQL —Ñ—É–Ω–∫—Ü–∏—é get_average_price_7days
            result = supabase.rpc(
                "get_average_price_7days",
                {"p_article_number": article, "p_days": 7}
            ).execute()
            
            if result.data and len(result.data) > 0:
                return result.data[0].get("avg_price")
            
            return None
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Failed to get average price from DB: {e}")
            return None
    
    async def get_product_stock(self, article: str) -> Optional[ProductStock]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Å—Ç–∞—Ç–∫–∞—Ö —Ç–æ–≤–∞—Ä–∞
        
        Args:
            article: –∞—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä–∞
            
        Returns:
            ProductStock –∏–ª–∏ None
        """
        product = await self.get_product_info(article)
        
        if not product:
            return None
        
        return ProductStock(
            article=article,
            availability=product.availability,
            stock_count=product.stock_count,
            last_updated=product.last_check
        )
    
    async def get_product_rating(self, article: str) -> Optional[ProductRating]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–π—Ç–∏–Ω–≥–µ —Ç–æ–≤–∞—Ä–∞
        
        Args:
            article: –∞—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä–∞
            
        Returns:
            ProductRating –∏–ª–∏ None
        """
        product = await self.get_product_info(article)
        
        if not product or not product.rating:
            return None
        
        return ProductRating(
            article=article,
            rating=product.rating,
            reviews_count=product.reviews_count or 0,
            last_updated=product.last_check
        )
    
    async def check_availability(self, article: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —Ç–æ–≤–∞—Ä–∞
        
        Args:
            article: –∞—Ä—Ç–∏–∫—É–ª —Ç–æ–≤–∞—Ä–∞
            
        Returns:
            True –µ—Å–ª–∏ —Ç–æ–≤–∞—Ä –≤ –Ω–∞–ª–∏—á–∏–∏
        """
        product = await self.get_product_info(article)
        return product.available if product else False
    
    async def scrape_multiple_products(
        self,
        articles: List[str],
        use_cache: bool = True
    ) -> Dict[str, Optional[ProductInfo]]:
        """
        Scrape –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–≤–∞—Ä–æ–≤ (batch processing)
        
        Args:
            articles: —Å–ø–∏—Å–æ–∫ –∞—Ä—Ç–∏–∫—É–ª–æ–≤
            use_cache: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {article: ProductInfo}
        """
        results = {}
        
        logger.info(f"üì¶ Batch scraping {len(articles)} products...")
        
        for article in articles:
            try:
                product = await self.get_product_info(article, use_cache=use_cache)
                results[article] = product
            except Exception as e:
                logger.error(f"‚ùå Failed to scrape {article}: {e}")
                results[article] = None
        
        successful = sum(1 for p in results.values() if p is not None)
        logger.info(f"‚úÖ Batch scraping completed: {successful}/{len(articles)} successful")
        
        return results
    
    # ==================== Statistics ====================
    
    def get_stats(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã scraper
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        cache_hit_rate = (
            (self.stats["cache_hits"] / max(self.stats["total_requests"], 1)) * 100
            if self.stats["total_requests"] > 0
            else 0
        )
        
        success_rate = (
            (self.stats["successful_requests"] / max(self.stats["total_requests"], 1)) * 100
            if self.stats["total_requests"] > 0
            else 0
        )
        
        return {
            **self.stats,
            "cache_size": len(self._cache),
            "cache_hit_rate": round(cache_hit_rate, 2),
            "success_rate": round(success_rate, 2)
        }
    
    def print_stats(self):
        """–í—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –∫–æ–Ω—Å–æ–ª—å"""
        stats = self.get_stats()
        
        logger.info("="*60)
        logger.info("üìä OZON Scraper Statistics:")
        logger.info("="*60)
        logger.info(f"Total requests: {stats['total_requests']}")
        logger.info(f"Successful: {stats['successful_requests']}")
        logger.info(f"Failed: {stats['failed_requests']}")
        logger.info(f"Success rate: {stats['success_rate']}%")
        logger.info(f"Cache hits: {stats['cache_hits']}")
        logger.info(f"Cache misses: {stats['cache_misses']}")
        logger.info(f"Cache hit rate: {stats['cache_hit_rate']}%")
        logger.info(f"Cache size: {stats['cache_size']}")
        logger.info(f"Playwright requests: {stats['playwright_requests']}")
        logger.info(f"httpx requests: {stats['httpx_requests']}")
        logger.info("="*60)


# ==================== Singleton ====================

_ozon_scraper_instance: Optional[OzonScraper] = None


def get_ozon_scraper() -> OzonScraper:
    """
    –ü–æ–ª—É—á–∏—Ç—å singleton —ç–∫–∑–µ–º–ø–ª—è—Ä OzonScraper
    
    Returns:
        OzonScraper instance
    """
    global _ozon_scraper_instance
    if _ozon_scraper_instance is None:
        _ozon_scraper_instance = OzonScraper(
            cache_ttl=getattr(settings, 'OZON_CACHE_TTL', 3600),
            rate_limit=getattr(settings, 'OZON_RATE_LIMIT', 10),
            timeout=getattr(settings, 'OZON_TIMEOUT', 30)
        )
    return _ozon_scraper_instance

