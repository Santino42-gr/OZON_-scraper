"""
Test script for OZON Scraper

–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ OzonScraper —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏.
"""

import asyncio
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º backend –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent))

from services.ozon_scraper import OzonScraper
from loguru import logger


async def test_basic_scraping():
    """–¢–µ—Å—Ç –±–∞–∑–æ–≤–æ–≥–æ scraping"""
    print("\n" + "="*60)
    print("üß™ Test 1: Basic Product Scraping")
    print("="*60)
    
    scraper = OzonScraper(cache_ttl=0)  # –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à –¥–ª—è —Ç–µ—Å—Ç–∞
    
    try:
        # –¢–µ—Å—Ç–æ–≤—ã–π –∞—Ä—Ç–∏–∫—É–ª (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π)
        test_article = "123456789"
        
        print(f"\nüì¶ Scraping product: {test_article}")
        
        product = await scraper.get_product_info(test_article, use_cache=False)
        
        if product:
            print(f"\n‚úÖ Product found:")
            print(f"   Article: {product.article}")
            print(f"   Name: {product.name}")
            print(f"   Price: {product.price} ‚ÇΩ")
            print(f"   Availability: {product.availability}")
            print(f"   Source: {product.source}")
            print(f"   Fetch time: {product.fetch_time_ms}ms")
        else:
            print(f"\n‚ö†Ô∏è  Product not found: {test_article}")
    
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        await scraper.close()


async def test_cache():
    """–¢–µ—Å—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("\n" + "="*60)
    print("üß™ Test 2: Caching")
    print("="*60)
    
    scraper = OzonScraper(cache_ttl=300)  # 5 –º–∏–Ω—É—Ç
    
    try:
        test_article = "123456789"
        
        # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å (–±–µ–∑ –∫—ç—à–∞)
        print(f"\n1Ô∏è‚É£  First request (no cache)...")
        from datetime import datetime
        start1 = datetime.now()
        product1 = await scraper.get_product_info(test_article)
        time1 = (datetime.now() - start1).total_seconds()
        print(f"   Time: {time1:.3f}s")
        
        # –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å (–∏–∑ –∫—ç—à–∞)
        print(f"\n2Ô∏è‚É£  Second request (from cache)...")
        start2 = datetime.now()
        product2 = await scraper.get_product_info(test_article)
        time2 = (datetime.now() - start2).total_seconds()
        print(f"   Time: {time2:.3f}s")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞
        if time2 < time1 * 0.1:  # –ö—ç—à –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ
            print(f"\n‚úÖ Cache test PASSED! Cache is {time1/time2:.1f}x faster")
        else:
            print(f"\n‚ö†Ô∏è  Cache might not be working properly")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = scraper.get_stats()
        print(f"\nüìä Stats:")
        print(f"   Cache hits: {stats['cache_hits']}")
        print(f"   Cache misses: {stats['cache_misses']}")
        print(f"   Cache hit rate: {stats['cache_hit_rate']}%")
    
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
    
    finally:
        await scraper.close()


async def test_rate_limiter():
    """–¢–µ—Å—Ç Rate Limiter"""
    print("\n" + "="*60)
    print("üß™ Test 3: Rate Limiter")
    print("="*60)
    
    from services.ozon_scraper import RateLimiter
    from datetime import datetime
    
    limiter = RateLimiter(max_requests=5, time_window=10)
    
    print(f"\n‚è±Ô∏è  Making 7 requests (limit: 5 req / 10s)...")
    
    start_time = datetime.now()
    
    for i in range(7):
        await limiter.acquire()
        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"   Request {i+1}/7 at {elapsed:.2f}s")
    
    total_time = (datetime.now() - start_time).total_seconds()
    
    print(f"\n‚úÖ Rate Limiter test completed!")
    print(f"   Total time: {total_time:.2f}s")
    print(f"   Expected: ~10s+ for 7 requests")


async def test_detailed_prices():
    """–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω"""
    print("\n" + "="*60)
    print("üß™ Test 4: Detailed Prices")
    print("="*60)
    
    scraper = OzonScraper()
    
    try:
        test_article = "123456789"
        
        print(f"\nüí∞ Getting detailed prices for: {test_article}")
        
        prices = await scraper.get_product_prices_detailed(test_article)
        
        if prices:
            print(f"\n‚úÖ Prices found:")
            print(f"   Article: {prices.article}")
            print(f"   Current price: {prices.price} ‚ÇΩ")
            print(f"   Normal price (no card): {prices.normal_price} ‚ÇΩ")
            print(f"   Ozon Card price: {prices.ozon_card_price} ‚ÇΩ")
            print(f"   Old price: {prices.old_price} ‚ÇΩ")
            print(f"   Average 7 days: {prices.average_price_7days} ‚ÇΩ")
        else:
            print(f"\n‚ö†Ô∏è  Prices not found")
    
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
    
    finally:
        await scraper.close()


async def test_batch_scraping():
    """–¢–µ—Å—Ç batch scraping"""
    print("\n" + "="*60)
    print("üß™ Test 5: Batch Scraping")
    print("="*60)
    
    scraper = OzonScraper()
    
    try:
        # –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∞—Ä—Ç–∏–∫—É–ª–æ–≤
        articles = ["123456789", "987654321", "111222333"]
        
        print(f"\nüì¶ Batch scraping {len(articles)} products...")
        
        results = await scraper.scrape_multiple_products(articles)
        
        print(f"\n‚úÖ Results:")
        for article, product in results.items():
            status = "‚úÖ Found" if product else "‚ùå Not found"
            print(f"   {article}: {status}")
    
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
    
    finally:
        await scraper.close()


async def test_statistics():
    """–¢–µ—Å—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    print("\n" + "="*60)
    print("üß™ Test 6: Statistics")
    print("="*60)
    
    scraper = OzonScraper()
    
    try:
        # –î–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤
        articles = ["123456789", "987654321"]
        
        for article in articles:
            await scraper.get_product_info(article)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print("\n")
        scraper.print_stats()
    
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
    
    finally:
        await scraper.close()


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("\n" + "="*80)
    print("üöÄ OZON Scraper - Complete Test Suite")
    print("="*80)
    
    try:
        # –¢–µ—Å—Ç 1: –ë–∞–∑–æ–≤—ã–π scraping
        await test_basic_scraping()
        
        # –¢–µ—Å—Ç 2: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        await test_cache()
        
        # –¢–µ—Å—Ç 3: Rate Limiter
        await test_rate_limiter()
        
        # –¢–µ—Å—Ç 4: –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã
        await test_detailed_prices()
        
        # –¢–µ—Å—Ç 5: Batch scraping
        await test_batch_scraping()
        
        # –¢–µ—Å—Ç 6: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        await test_statistics()
        
        print("\n" + "="*80)
        print("‚úÖ ALL TESTS COMPLETED!")
        print("="*80)
        print("\n‚ö†Ô∏è  NOTE: Some tests may fail if test articles don't exist on OZON")
        print("‚ö†Ô∏è  Replace test articles with real OZON product IDs for accurate testing")
        print("\n")
        
    except KeyboardInterrupt:
        print("\n\n‚è∏Ô∏è  Tests interrupted by user")
    except Exception as e:
        print(f"\n\n‚ùå TEST SUITE FAILED: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
    asyncio.run(main())

